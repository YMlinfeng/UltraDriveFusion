sp_vae:magicdrivedit/utils/train_utils.py
æˆ‘ä»¬æ¥ **é€è¡Œè¯¦ç»†åˆ†æ VAE ç¼–ç éƒ¨åˆ†çš„é€»è¾‘**ï¼ŒåŒ…æ‹¬ï¼š

- Python è¯­æ³•è§£é‡Š
- æ¯ä¸ªå˜é‡çš„å«ä¹‰
- æ¯ä¸ªå‡½æ•°å’Œç±»çš„åŠŸèƒ½
- PyTorch å’Œåˆ†å¸ƒå¼é€šä¿¡çš„æœºåˆ¶

---

## ğŸ§  èƒŒæ™¯è¯´æ˜

åœ¨è¿™ä¸ªè®­ç»ƒæ¡†æ¶ä¸­ï¼Œ**VAE**ï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰è¢«ç”¨æ¥å°†è¾“å…¥è§†é¢‘å¸§ï¼ˆå›¾åƒï¼‰ç¼–ç æˆå‹ç¼©çš„ latent è¡¨ç¤ºã€‚ç”±äºæ¨¡å‹å¯èƒ½æå¤§ã€æ•°æ®é‡åºå¤§ï¼Œæˆ‘ä»¬ä½¿ç”¨ **åºåˆ—å¹¶è¡Œï¼ˆSequence Parallelismï¼‰** æŠ€æœ¯å°†ç¼–ç ä»»åŠ¡åˆ†å¸ƒåˆ°å¤šä¸ªè¿›ç¨‹ä¸Šå¹¶è¡Œæ‰§è¡Œ â€”â€” è¿™å°±æ˜¯ `sp_vae` çš„ä½œç”¨ã€‚

---

## ğŸ§© ä¸»è°ƒç”¨è¯­å¥åˆ†æ

```python
with RandomStateManager(verbose=verbose_mode):
    x = sp_vae(x, vae.encode, get_sequence_parallel_group())
```

### âœ… è§£é‡Šæ¯ä¸ªç»„ä»¶ï¼š

| ç»„ä»¶ | ç±»å‹ | ä½œç”¨ |
|------|------|------|
| `RandomStateManager` | ä¸Šä¸‹æ–‡ç®¡ç†å™¨ | æ§åˆ¶ PyTorch éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€ï¼Œç¡®ä¿ **å¹¶è¡Œè®¡ç®—çš„ç¡®å®šæ€§** |
| `x` | `torch.Tensor` | è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸€èˆ¬æ˜¯ `(B * NC, C, T, H, W)`ï¼Œè§†é¢‘å¸§æ•°æ® |
| `vae.encode` | å‡½æ•° | ç”¨äºå°†å›¾åƒç¼–ç æˆ latent è¡¨è¾¾ |
| `get_sequence_parallel_group()` | å‡½æ•° | è·å– ColossalAI æ³¨å†Œçš„ "sequence" å¹¶è¡Œè¿›ç¨‹ç»„ï¼ˆ`ProcessGroup`ï¼‰|

---

## ğŸ” `get_sequence_parallel_group()` å‡½æ•°

```python
def get_sequence_parallel_group():
    group = _GLOBAL_PARALLEL_GROUPS.get("sequence", None)
    if group == None:
        raise RuntimeError("sequence_parallel_group is None")
    return group
```

### âœ… è§£é‡Šï¼š

- `_GLOBAL_PARALLEL_GROUPS` æ˜¯ä¸€ä¸ªå…¨å±€å­—å…¸ï¼Œç»´æŠ¤äº†ä¸åŒç±»å‹çš„å¹¶è¡Œè¿›ç¨‹ç»„ï¼ˆå¦‚æ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€åºåˆ—å¹¶è¡Œç­‰ï¼‰ã€‚
- `"sequence"` æ˜¯é”®ï¼ˆkeyï¼‰ï¼Œè¡¨ç¤ºè¿™æ˜¯ç”¨äºå¤„ç†åºåˆ—å¹¶è¡Œä»»åŠ¡çš„è¿›ç¨‹ç»„ã€‚
- å¦‚æœæ²¡æœ‰æ³¨å†Œè¯¥ç»„ï¼Œå°±æŠ›å‡ºå¼‚å¸¸ã€‚

---

## ğŸ” `sp_vae` å‡½æ•°è¯¦è§£

```python
def sp_vae(x, vae_func, sp_group: dist.ProcessGroup):
```

- `x`: è¾“å…¥å›¾åƒæ•°æ® `(B * NC) x C x T x H x W`
- `vae_func`: VAE çš„ç¼–ç å‡½æ•°ï¼Œå¦‚ `vae.encode`
- `sp_group`: åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼Œç”¨äº sequence parallelism

---

### 1. è·å–è¿›ç¨‹ç»„ä¿¡æ¯

```python
group_size = dist.get_world_size(sp_group)
local_rank = dist.get_rank(sp_group)
B = x.shape[0]
```

- `group_size`: å½“å‰å¹¶è¡Œç»„ä¸­è¿›ç¨‹æ•°é‡
- `local_rank`: å½“å‰è¿›ç¨‹åœ¨è¯¥ç»„ä¸­çš„ rank
- `B`: è¾“å…¥æ•°æ®çš„ batch sizeï¼ˆæ³¨æ„ï¼šå·²ç»åˆå¹¶äº† batch å’Œç›¸æœºç»´åº¦ï¼‰

---

### 2. è®¡ç®—æ¯ä¸ªè¿›ç¨‹å¤„ç†å¤šå°‘æ ·æœ¬

```python
copy_size = group_size
while copy_size < B:
    copy_size += group_size
per_rank_bs = copy_size // group_size
```

- `copy_size`: ç›®æ ‡ batch sizeï¼Œæ˜¯ group size çš„æ•´æ•°å€ï¼Œä¾¿äºå‡åŒ€åˆ†é…
- `per_rank_bs`: æ¯ä¸ªè¿›ç¨‹è¦å¤„ç†å¤šå°‘æ ·æœ¬

**ç›®çš„ï¼šä¿è¯æ¯ä¸ª rank æ‹¿åˆ°ä¸€æ ·å¤šçš„æ•°æ®ï¼Œä¾¿äº all_gather æ‹¼æ¥**

---

### 3. æ•°æ®æ‰©å±•ï¼ˆpadï¼‰æˆ–æŠ¥é”™ä¿æŠ¤

```python
if per_rank_bs >= B:
    warn_once(...)  # æ‰“æ—¥å¿—
    return vae_func(x)
```

- å¦‚æœ batch å¤ªå°ï¼Œä¸è¶³ä»¥å¹¶è¡Œå¤„ç†ï¼Œå°± fallbackï¼ˆå›é€€ï¼‰åˆ°å•è¿›ç¨‹å¤„ç†ã€‚

```python
if copy_size > B:
    x_copy_num = math.ceil(copy_size / B)
    x_temp = torch.cat([x for _ in range(x_copy_num)])[:copy_size]
```

- å¦‚æœå¤åˆ¶æ•°é‡ä¸å¤Ÿï¼Œé€šè¿‡ `torch.cat` é‡å¤ x æ¥æ‰©å±•ï¼Œç›´åˆ°æ»¡è¶³ `copy_size`

```python
elif copy_size < B:
    raise RuntimeError(...)
```

- ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œå¦‚æœå‘ç”Ÿè¯´æ˜è®¡ç®—æœ‰é—®é¢˜

---

### 4. åˆ‡åˆ†æ¯ä¸ªè¿›ç¨‹çš„æ•°æ®

```python
local_x = x_temp[local_rank * per_rank_bs:(local_rank + 1) * per_rank_bs]
assert local_x.shape[0] == per_rank_bs
```

- æ¯ä¸ªè¿›ç¨‹ä» `x_temp` ä¸­æ‹¿è‡ªå·±åº”è¯¥å¤„ç†çš„éƒ¨åˆ†ã€‚
- assert æ˜¯ä¸€ç§ä¿æŠ¤æœºåˆ¶ï¼Œä¿è¯åˆ‡åˆ†æ­£ç¡®ã€‚

---

### 5. å±€éƒ¨ VAE ç¼–ç 

```python
local_latent = vae_func(local_x)
```

- æ¯ä¸ªè¿›ç¨‹å¯¹è‡ªå·±çš„æ•°æ® `local_x` æ‰§è¡Œ VAE ç¼–ç ï¼Œå¾—åˆ° `local_latent`
- å½¢çŠ¶ä¸º `[per_rank_bs, C_latent, T, H', W']`

---

### 6. ä½¿ç”¨ `all_gather` èšåˆæ‰€æœ‰è¿›ç¨‹ç»“æœ

```python
global_latent = [torch.empty_like(local_latent) for _ in range(group_size)]
dist.all_gather(global_latent, local_latent, group=sp_group)
dist.barrier(sp_group)
```

- åˆ›å»ºä¸€ä¸ª list å­˜æ”¾å…¶ä»–è¿›ç¨‹çš„ latent
- ä½¿ç”¨ `dist.all_gather` å°†æ¯ä¸ªè¿›ç¨‹çš„ latent æ”¶é›†åˆ°æ¯ä¸ªè¿›ç¨‹
- `dist.barrier` ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®ŒæˆåŒæ­¥

---

### 7. æ‹¼æ¥æ‰€æœ‰ latent å¹¶è£å‰ªå›åŸå§‹é•¿åº¦

```python
global_latent = torch.cat(global_latent, dim=0)[:B]
```

- æ‹¼æ¥æ‰€æœ‰è¿›ç¨‹çš„ latentï¼Œå¾—åˆ°å®Œæ•´ç»“æœ
- è‹¥ä¹‹å‰æœ‰ paddingï¼Œè¿™é‡Œè£å‰ªå›æ¥

---

### 8. è¿”å›æœ€ç»ˆ latent è¡¨è¾¾

```python
return global_latent
```

- è¿”å›ç¼–ç åçš„ç»“æœï¼Œå½¢çŠ¶ä¸º `[B, ...]`ï¼Œä¸è¾“å…¥ batch å¤§å°ä¸€è‡´

---

## ğŸ“Œ æ€»ç»“ï¼šVAE ç¼–ç é€»è¾‘æµç¨‹å›¾

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input x     â”‚  <- shape: (B*NC, C, T, H, W)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Check if sp_vae is needed
     â†“
Pad x to divisible by world_size
     â†“
Split x by rank -> local_x
     â†“
local_latent = VAE.encode(local_x)
     â†“
All-gather all local_latent -> global_latent
     â†“
Trim to original B
     â†“
Return latent feature
```

---

## ğŸš€ æ€»ç»“å½’çº³

| æ­¥éª¤ | å†…å®¹ | è¯´æ˜ |
|------|------|------|
| `vae.encode(x)` | ç¼–ç å›¾åƒå¸§ä¸º latent è¡¨è¾¾ | ç”¨äºè¾“å…¥ diffusion æ¨¡å‹ |
| `RandomStateManager` | æ§åˆ¶éšæœºæ€§ | ä¿è¯å¹¶è¡Œç¯å¢ƒä¸‹éšæœºæ•°ä¸€è‡´ |
| `sp_vae` | åˆ†å¸ƒå¼å¹¶è¡Œ VAE ç¼–ç  | é¿å…å•å¡å†…å­˜ç“¶é¢ˆï¼Œæé«˜æ•ˆç‡ |
| `all_gather` | å¤šè¿›ç¨‹é€šä¿¡ | æ”¶é›†æ¯ä¸ª rank çš„ latentï¼Œç»„åˆå®Œæ•´ batch |
| `get_sequence_parallel_group` | è·å–è¿›ç¨‹ç»„ | å®ç° Sequence Parallelism |

---

è¿˜æƒ³äº†è§£ï¼š

- `vae.encode` é‡Œé¢å…·ä½“åšäº†ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•å®šä¹‰ `VAE` æ¨¡å‹ï¼Ÿ
- ä¸ºä»€ä¹ˆéœ€è¦ `RandomStateManager` ä¿è¯ç¡®å®šæ€§ï¼Ÿ
